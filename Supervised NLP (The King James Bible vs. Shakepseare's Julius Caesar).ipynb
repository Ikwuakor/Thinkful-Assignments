{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised NLP (The King James Bible vs. Shakepseare's)\n",
    "+ pre-labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "shakespeare = gutenberg.raw('shakespeare-caesar.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "bible = re.sub(r'Chapter \\d+', '', bible)\n",
    "bible = re.sub(r'\\d+:\\d+', '', bible)\n",
    "shakespeare = re.sub(r'CHAPTER .*', '', shakespeare)\n",
    "    \n",
    "shakespeare = text_cleaner(shakespeare[:int(len(shakespeare)/10)])\n",
    "bible = text_cleaner(bible[:int(len(bible)/400)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10322"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10963"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "bible_doc = nlp(bible)\n",
    "shakespeare_doc = nlp(shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(The, Old, Testament, of, the, King, James, Bi...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(The, First, Book, of, Moses, :, Called, Genesis)</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(In, the, beginning, God, created, the, heaven...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(And, the, earth, was, without, form, ,, and, ...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(And, the, Spirit, of, God, moved, upon, the, ...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0    1\n",
       "0  (The, Old, Testament, of, the, King, James, Bi...  KJV\n",
       "1  (The, First, Book, of, Moses, :, Called, Genesis)  KJV\n",
       "2  (In, the, beginning, God, created, the, heaven...  KJV\n",
       "3  (And, the, earth, was, without, form, ,, and, ...  KJV\n",
       "4  (And, the, Spirit, of, God, moved, upon, the, ...  KJV"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "bible_sents = [[sent, \"KJV\"] for sent in bible_doc.sents]\n",
    "shakespeare_sents = [[sent, \"Shakespeare\"] for sent in shakespeare_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(bible_sents + shakespeare_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "biblewords = bag_of_words(bible_doc)\n",
    "shakespearewords = bag_of_words(shakespeare_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(biblewords + shakespearewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heap'd</th>\n",
       "      <th>second</th>\n",
       "      <th>turne</th>\n",
       "      <th>midst</th>\n",
       "      <th>evening</th>\n",
       "      <th>loose</th>\n",
       "      <th>Bible</th>\n",
       "      <th>head</th>\n",
       "      <th>Prima</th>\n",
       "      <th>bring</th>\n",
       "      <th>...</th>\n",
       "      <th>alas</th>\n",
       "      <th>Hast</th>\n",
       "      <th>herb</th>\n",
       "      <th>Calphurnia</th>\n",
       "      <th>light</th>\n",
       "      <th>thought</th>\n",
       "      <th>lose</th>\n",
       "      <th>heauy</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, Old, Testament, of, the, King, James, Bi...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, First, Book, of, Moses, :, Called, Genesis)</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(In, the, beginning, God, created, the, heaven...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, the, earth, was, without, form, ,, and, ...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, the, Spirit, of, God, moved, upon, the, ...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 792 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  heap'd second turne midst evening loose Bible head Prima bring  ... alas  \\\n",
       "0      0      0     0     0       0     0     1    0     0     0  ...    0   \n",
       "1      0      0     0     0       0     0     0    0     0     0  ...    0   \n",
       "2      0      0     0     0       0     0     0    0     0     0  ...    0   \n",
       "3      0      0     0     0       0     0     0    0     0     0  ...    0   \n",
       "4      0      0     0     0       0     0     0    0     0     0  ...    0   \n",
       "\n",
       "  Hast herb Calphurnia light thought lose heauy  \\\n",
       "0    0    0          0     0       0    0     0   \n",
       "1    0    0          0     0       0    0     0   \n",
       "2    0    0          0     0       0    0     0   \n",
       "3    0    0          0     0       0    0     0   \n",
       "4    0    0          0     0       0    0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (The, Old, Testament, of, the, King, James, Bi...         KJV  \n",
       "1  (The, First, Book, of, Moses, :, Called, Genesis)         KJV  \n",
       "2  (In, the, beginning, God, created, the, heaven...         KJV  \n",
       "3  (And, the, earth, was, without, form, ,, and, ...         KJV  \n",
       "4  (And, the, Spirit, of, God, moved, upon, the, ...         KJV  \n",
       "\n",
       "[5 rows x 792 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.994475138121547\n",
      "\n",
      "Test set score: 0.9590163934426229\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9779005524861878\n",
      "\n",
      "Test set score: 0.9672131147540983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>KJV</th>\n",
       "      <th>Shakespeare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KJV</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shakespeare</th>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        KJV  Shakespeare\n",
       "text_source                  \n",
       "KJV           53            4\n",
       "Shakespeare    0          124"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_train, lr.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.0\n",
      "\n",
      "Test set score: 0.9590163934426229\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  0.9229588431590656\n",
      "Logistic Regression:  0.9163922877271042\n",
      "Gradient Boosting:  0.9128364849833149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print('Random Forest: ', cross_val_score(rfc, X, Y, cv=10).mean())\n",
    "print('Logistic Regression: ', cross_val_score(lr, X, Y, cv=10).mean())\n",
    "print('Gradient Boosting: ', cross_val_score(clf, X, Y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like Random Forest is the winner! Let's see if we can improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.994475138121547\n",
      "\n",
      "Test set score: 0.9672131147540983\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  0.9128364849833147\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest: ', cross_val_score(rfc, X, Y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from tqdm import tqdm # show progress bar\n",
    "import time # time random searches\n",
    "\n",
    "Y = pd.get_dummies(Y, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#########    Creating a function in order to quickly call upon    ######\n",
    "#########         the model with different features  while        ######\n",
    "#########        cycling through different model parameters       ######\n",
    "########################################################################\n",
    "\n",
    "def RandForest_RandomSearch(X, Y, criterion=['gini', 'entropy'],\n",
    "                                  # max_depth=[3, 4, 5],\n",
    "                                  # n_estimators=[50, 100, 250],\n",
    "                                  class_weight=['balanced', 'balanced_subsample', None]\n",
    "                           ):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "    rfc_hyperparams = dict(\n",
    "                           criterion = criterion,\n",
    "                           # max_depth = max_depth,\n",
    "                           # n_estimators = n_estimators,\n",
    "                           class_weight = class_weight\n",
    "                           )\n",
    "\n",
    "    rfc_hyper_table = list()\n",
    "\n",
    "    # Timed for loop...start the clock!\n",
    "    start_time = time.time()\n",
    "    for trial in tqdm(range(10), position=0):\n",
    "        params = dict()\n",
    "        for k, v in rfc_hyperparams.items():\n",
    "            i = np.random.randint(len(v))\n",
    "            params[k] = v[i]\n",
    "        #print(params)\n",
    "        rfc = ensemble.RandomForestClassifier(**params)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        y_pred_train = rfc.predict(X_train)\n",
    "        y_pred_test = rfc.predict(X_test)\n",
    "        params['train_precision'] = precision_score(y_train, y_pred_train)\n",
    "        params['test_precision'] = precision_score(y_test, y_pred_test)\n",
    "        params['train_recall'] = recall_score(y_train, y_pred_train)\n",
    "        params['test_recall'] = recall_score(y_test, y_pred_test)\n",
    "        params['train_acc'] = rfc.score(X_train, y_train)\n",
    "        params['test_acc'] = rfc.score(X_test, y_test)\n",
    "        params['train_f1'] = 2 * (params['train_precision'] * params['train_recall'])/(params['train_precision'] + params['train_recall'])\n",
    "        params['test_f1'] = 2 * (params['test_precision'] * params['test_recall'])/(params['test_precision'] + params['test_recall'])\n",
    "        params['cross_val_mean'] = cross_val_score(rfc, X, Y, cv=10).mean()\n",
    "        # Store our parameters and score in a dataframe\n",
    "        rfc_hyper_table.append(params)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "    print(f'Execution time: {round(end_time - start_time, 2)} seconds')\n",
    "    rfc_hyper_table = pd.DataFrame(data=rfc_hyper_table)\n",
    "    return rfc_hyper_table.sort_values(by='cross_val_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col10 {\n",
       "            background:  yellow;\n",
       "        }    #T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col10 {\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001f\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >criterion</th>        <th class=\"col_heading level0 col1\" >class_weight</th>        <th class=\"col_heading level0 col2\" >train_precision</th>        <th class=\"col_heading level0 col3\" >test_precision</th>        <th class=\"col_heading level0 col4\" >train_recall</th>        <th class=\"col_heading level0 col5\" >test_recall</th>        <th class=\"col_heading level0 col6\" >train_acc</th>        <th class=\"col_heading level0 col7\" >test_acc</th>        <th class=\"col_heading level0 col8\" >train_f1</th>        <th class=\"col_heading level0 col9\" >test_f1</th>        <th class=\"col_heading level0 col10\" >cross_val_mean</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col0\" class=\"data row0 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col1\" class=\"data row0 col1\" >balanced_subsample</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col2\" class=\"data row0 col2\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col3\" class=\"data row0 col3\" >0.944444</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col4\" class=\"data row0 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col5\" class=\"data row0 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col6\" class=\"data row0 col6\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col7\" class=\"data row0 col7\" >0.956044</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col8\" class=\"data row0 col8\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col9\" class=\"data row0 col9\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow0_col10\" class=\"data row0 col10\" >0.933626</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row1\" class=\"row_heading level0 row1\" >3</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col0\" class=\"data row1 col0\" >gini</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col1\" class=\"data row1 col1\" >balanced</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col2\" class=\"data row1 col2\" >0.986577</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col3\" class=\"data row1 col3\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col4\" class=\"data row1 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col5\" class=\"data row1 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col6\" class=\"data row1 col6\" >0.990566</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col7\" class=\"data row1 col7\" >0.978022</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col8\" class=\"data row1 col8\" >0.993243</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col9\" class=\"data row1 col9\" >0.985507</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow1_col10\" class=\"data row1 col10\" >0.930515</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col0\" class=\"data row2 col0\" >gini</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col1\" class=\"data row2 col1\" >balanced</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col2\" class=\"data row2 col2\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col3\" class=\"data row2 col3\" >0.944444</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col4\" class=\"data row2 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col5\" class=\"data row2 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col6\" class=\"data row2 col6\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col7\" class=\"data row2 col7\" >0.956044</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col8\" class=\"data row2 col8\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col9\" class=\"data row2 col9\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow2_col10\" class=\"data row2 col10\" >0.93007</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row3\" class=\"row_heading level0 row3\" >4</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col0\" class=\"data row3 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col1\" class=\"data row3 col1\" >balanced</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col2\" class=\"data row3 col2\" >0.993243</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col3\" class=\"data row3 col3\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col4\" class=\"data row3 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col5\" class=\"data row3 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col6\" class=\"data row3 col6\" >0.995283</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col7\" class=\"data row3 col7\" >0.978022</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col8\" class=\"data row3 col8\" >0.99661</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col9\" class=\"data row3 col9\" >0.985507</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow3_col10\" class=\"data row3 col10\" >0.923396</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row4\" class=\"row_heading level0 row4\" >7</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col0\" class=\"data row4 col0\" >gini</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col1\" class=\"data row4 col1\" >balanced</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col2\" class=\"data row4 col2\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col3\" class=\"data row4 col3\" >0.944444</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col4\" class=\"data row4 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col5\" class=\"data row4 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col6\" class=\"data row4 col6\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col7\" class=\"data row4 col7\" >0.956044</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col8\" class=\"data row4 col8\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col9\" class=\"data row4 col9\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow4_col10\" class=\"data row4 col10\" >0.920178</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row5\" class=\"row_heading level0 row5\" >6</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col0\" class=\"data row5 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col1\" class=\"data row5 col1\" >None</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col2\" class=\"data row5 col2\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col3\" class=\"data row5 col3\" >0.944444</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col4\" class=\"data row5 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col5\" class=\"data row5 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col6\" class=\"data row5 col6\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col7\" class=\"data row5 col7\" >0.956044</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col8\" class=\"data row5 col8\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col9\" class=\"data row5 col9\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow5_col10\" class=\"data row5 col10\" >0.919841</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row6\" class=\"row_heading level0 row6\" >8</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col0\" class=\"data row6 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col1\" class=\"data row6 col1\" >balanced_subsample</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col2\" class=\"data row6 col2\" >0.993243</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col3\" class=\"data row6 col3\" >0.944444</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col4\" class=\"data row6 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col5\" class=\"data row6 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col6\" class=\"data row6 col6\" >0.995283</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col7\" class=\"data row6 col7\" >0.956044</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col8\" class=\"data row6 col8\" >0.99661</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col9\" class=\"data row6 col9\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow6_col10\" class=\"data row6 col10\" >0.91706</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row7\" class=\"row_heading level0 row7\" >0</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col0\" class=\"data row7 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col1\" class=\"data row7 col1\" >None</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col2\" class=\"data row7 col2\" >0.993243</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col3\" class=\"data row7 col3\" >0.931507</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col4\" class=\"data row7 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col5\" class=\"data row7 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col6\" class=\"data row7 col6\" >0.995283</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col7\" class=\"data row7 col7\" >0.945055</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col8\" class=\"data row7 col8\" >0.99661</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col9\" class=\"data row7 col9\" >0.964539</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow7_col10\" class=\"data row7 col10\" >0.91673</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row8\" class=\"row_heading level0 row8\" >9</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col0\" class=\"data row8 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col1\" class=\"data row8 col1\" >None</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col2\" class=\"data row8 col2\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col3\" class=\"data row8 col3\" >0.944444</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col4\" class=\"data row8 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col5\" class=\"data row8 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col6\" class=\"data row8 col6\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col7\" class=\"data row8 col7\" >0.956044</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col8\" class=\"data row8 col8\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col9\" class=\"data row8 col9\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow8_col10\" class=\"data row8 col10\" >0.909711</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001flevel0_row9\" class=\"row_heading level0 row9\" >1</th>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col0\" class=\"data row9 col0\" >entropy</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col1\" class=\"data row9 col1\" >balanced_subsample</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col2\" class=\"data row9 col2\" >0.993243</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col3\" class=\"data row9 col3\" >0.971429</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col4\" class=\"data row9 col4\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col5\" class=\"data row9 col5\" >1</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col6\" class=\"data row9 col6\" >0.995283</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col7\" class=\"data row9 col7\" >0.978022</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col8\" class=\"data row9 col8\" >0.99661</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col9\" class=\"data row9 col9\" >0.985507</td>\n",
       "                        <td id=\"T_e76d5fbc_b42f_11ea_9965_9cb6d09a001frow9_col10\" class=\"data row9 col10\" >0.896923</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7efbca153b70>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the Random Forest function\n",
    "rand_forest_df = RandForest_RandomSearch(X,Y)\n",
    "rand_forest_df.style.apply(lambda x: ['background: yellow' if x.name == 'cross_val_mean' else '' for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We've managed to get a slight improvement on our mean cross validation score with consistently high F1 scores by changing the criterion to entropy and using a 'balanced_subsample' class_weight.\n",
    "\n",
    "+ Results may vary on consecutive execution of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion                     entropy\n",
      "class_weight       balanced_subsample\n",
      "train_precision                     1\n",
      "test_precision               0.944444\n",
      "train_recall                        1\n",
      "test_recall                         1\n",
      "train_acc                           1\n",
      "test_acc                     0.956044\n",
      "train_f1                            1\n",
      "test_f1                      0.971429\n",
      "cross_val_mean               0.933626\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Results \n",
    "print(rand_forest_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
